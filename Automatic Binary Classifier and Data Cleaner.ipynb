{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-38842dddf464>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from xgboost import XGBClassifier\n",
    "import statistics\n",
    "import shap\n",
    "import numpy.core.multiarray\n",
    "\n",
    "\n",
    "#return data before number\n",
    "def beforenum(data):\n",
    "    data=str(data)\n",
    "    for i in range(len(data)):\n",
    "        if unicode(str(data[i])).isnumeric():\n",
    "            return data[:i]\n",
    "    return data\n",
    "\n",
    "\n",
    "#return data before number\n",
    "def onlyint(data):\n",
    "    data=str(data)\n",
    "    for i in range(len(data)):\n",
    "        if ~unicode(str(data[i])).isnumeric():\n",
    "            #print ('yes')\n",
    "            return data[:i]\n",
    "    return data\n",
    "\n",
    "#return float before text\n",
    "def onlyfloat(data):\n",
    "    data=str(data)\n",
    "    for i in range(len(data)):\n",
    "        if ~unicode(str(data[i])).isnumeric() and str(data[i])!='.':\n",
    "            #print ('yes')\n",
    "            return data[:i]\n",
    "        i=i+1\n",
    "    return data\n",
    "\n",
    "\n",
    "#normalize the dataframe\n",
    "def normalizedataframe(df):\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(df.values)\n",
    "    return pd.DataFrame(x_scaled,columns=df.columns.values)\n",
    "\n",
    "# if one row is a category and another corresponding row is the value turns that category into a columna and puts the data in it\n",
    "# example lab test| value| patient\n",
    "#         flu     | 3    |1\n",
    "#         fever   | 2   |1\n",
    "#         flu    | 4    |2\n",
    "#   patient| flu| fever\n",
    "#    1| 3| 2\n",
    "#    1| 4 | null\n",
    "def catvalueget(data,catcol,valcat,keep,joinonleft,joinonright,minc):\n",
    "    test=data\n",
    "    names=data[catcol].value_counts().index\n",
    "    l=len(names)\n",
    "    #returns only categories with at least x amount of unique values,\n",
    "    l=mincount(data,catcol,minc)\n",
    "    i=0\n",
    "    #creates column with \n",
    "    while i<l:\n",
    "        value=names[i]\n",
    "        value=str(value)\n",
    "        #test[value]=0\n",
    "        test2=test[test[catcol]==value]\n",
    "        test2[value]=0\n",
    "        keep=list(keep)\n",
    "        keep.append(valcat)\n",
    "        test2=test2[keep]\n",
    "        test2=test2.rename(columns={valcat:value})\n",
    "        test = pd.merge(test, test2,  how='left', left_on=joinonleft, right_on = joinonright)\n",
    "        test = test.loc[:,~test.columns.duplicated()]\n",
    "        i=i+1\n",
    "    test = test.loc[:,~test.columns.duplicated()]\n",
    "    return test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# remove all columns that can't be turned into float\n",
    "def removenonnumcols(data):\n",
    "    cols=data.columns.values\n",
    "    for col in cols:\n",
    "        try:\n",
    "            data[col]=data[col].astype(float)\n",
    "        except:\n",
    "            data=data.drop(columns=[col])\n",
    "        i=i+1\n",
    "    data=data.fillna(-1)\n",
    "    return data\n",
    "        \n",
    "\n",
    "#return place that value count is lower than min count\n",
    "def mincount(data,col,Min):\n",
    "    lis=data[col].value_counts()\n",
    "    i=0\n",
    "    l=len(lis)\n",
    "    while i<l:\n",
    "        amount=float(lis[i])\n",
    "        if amount<Min:\n",
    "            return i\n",
    "        i=i+1\n",
    "        \n",
    "    i=i-1\n",
    "    return i\n",
    "\n",
    "# fits the data to logistic regression to get weights of features and see how they perform\n",
    "# write the feature weights to file for you to view\n",
    "# returns weights * coef_ and feature name\n",
    "def simplefeatureselection(data,predict,targetlist,weightfile):\n",
    "    if len(targetlist)!=len(predict.value_counts().index):\n",
    "        print ('Targetlist is Length: '+str(len(targetlist))+\" But the length should be: \"+str(len(predict.value_counts().index)))\n",
    "    clf=LR()\n",
    "    clf=clf.fit(data,predict)\n",
    "    featureweights=np.std(data.values, 0)*clf.coef_\n",
    "    temp = pd.DataFrame(featureweights, columns=data.columns.values)\n",
    "    temp.T.to_csv(weightfile)\n",
    "    temp['Target']=targetlist\n",
    "    return temp\n",
    "\n",
    "# gets top amount of features based on average weight for that feature, and returns those features to limit the amount of features\n",
    "def rankingfeatures(data,target,top):\n",
    "    temp=data\n",
    "    temp2=temp.set_index(target)\n",
    "    tempabs=abs(temp2)\n",
    "    tempabs=tempabs.T\n",
    "    l=len(tempabs.columns.values)\n",
    "    i=0\n",
    "    topamount=top\n",
    "    while i<l:\n",
    "        if True:\n",
    "            tempabs=tempabs.sort_values(by=tempabs.columns.values[i],ascending=False)\n",
    "            order= (tempabs.index[:topamount])    \n",
    "            temptemp=temp2[temp2.index==tempabs.columns.values[i]].reset_index()\n",
    "            temptemp= (temptemp[order])\n",
    "            temptempval=temptemp.values\n",
    "            if i==0:\n",
    "                extradataframe=pd.DataFrame({str(tempabs.columns.values[i])+'_Weights':temptempval[0],str(tempabs.columns.values[i])+'_Names':order})\n",
    "            if True:\n",
    "                extradataframe[str(tempabs.columns.values[i])+'_Weights']=list(temptempval[0])\n",
    "                extradataframe[str(tempabs.columns.values[i])+'_Names']=order\n",
    "\n",
    "        i=i+1\n",
    "    return extradataframe\n",
    "\n",
    "\n",
    "#returns important features based on top x amount\n",
    "def getfeaturesback(data,predict,targetlist,weightfile,top):\n",
    "    try:\n",
    "        predict2=predict\n",
    "        predict=data[predict]\n",
    "        data=data.drop(columns=predict2)\n",
    "    except:\n",
    "        True\n",
    "    if True:\n",
    "        temp=simplefeatureselection(data,predict,targetlist,weightfile)\n",
    "        temp=rankingfeatures(temp,'Target',top)\n",
    "    return temp\n",
    "\n",
    "\n",
    "# get columns in the data and how often they occur with the specific ending\n",
    "def commonfeatures(data,ending='_Names'):\n",
    "    data = data.filter(regex=ending)\n",
    "    cols=data.columns.values\n",
    "    items=[]\n",
    "    for col in cols:\n",
    "        items=items+(list(data[col].values[:]))\n",
    "    myset = list(set(li))\n",
    "    countlist=[]\n",
    "    for item in myset:\n",
    "        countlist.append(items.count(item))\n",
    "        i=i+1\n",
    "    df=pd.DataFrame({'COL':myset,'Count':countlist})\n",
    "    return df\n",
    "\n",
    "\n",
    "#write the performance of the XGBoost model\n",
    "def writeperformance(data,predicted,test,file):\n",
    "    try:\n",
    "        predict2=predicted\n",
    "        predicted=data[predicted]\n",
    "        data=data.drop(columns=predict2)\n",
    "    except:\n",
    "        True\n",
    "    data['Predictval']=predicted\n",
    "    f=open(file,'w')\n",
    "    f.write(\"CUTOFF\\tTP\\tFP\\tTN\\tFN\\n\")\n",
    "    cutoff=0\n",
    "    while cutoff<1:\n",
    "        mask=data['Predictval']>=cutoff\n",
    "        data[\"Value\"]=0\n",
    "        data.loc[mask,'Value']=1\n",
    "        confusionmatrix=confusion_matrix(data[test],data['Value'])\n",
    "        tn = confusionmatrix [0][0]\n",
    "        fn = confusionmatrix [1][0]\n",
    "        tp = confusionmatrix [1][1]\n",
    "        fp = confusionmatrix [0][1]\n",
    "        f.write(str(cutoff)+\"\\t\"+str(tp)+\"\\t\"+str(fp)+\"\\t\"+str(tn)+\"\\t\"+str(fn)+\"\\n\")\n",
    "        cutoff=cutoff+.01\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "#return list of features that appear x amount of times or greater in the list\n",
    "def minocc(coldata,cutoff,colcol='COL',cutoffcol='Count'):\n",
    "    coldata=coldata[coldata[cutoffcol]>cutoff]\n",
    "    return list(coldata[colcol].value_counts().index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#trains the XGBoost model\n",
    "def trainxgboost(data,cols,predict,md=3,ne=1000,g=5,lr=.1):\n",
    "    try:\n",
    "        predict2=predict\n",
    "        predict=data[predict]\n",
    "        data=data.drop(columns=predict2)\n",
    "    except:\n",
    "        True\n",
    "    \n",
    "    clf=XGBClassifier(max_depth=md,n_estimators=ne,gamma=g,learning_rate=lr)\n",
    "    return clf.fit(data[cols],predict)\n",
    "    \n",
    "\n",
    "    \n",
    "#split train test data to train and predict on on an XGBoost Model\n",
    "def splittest(data,cols,predict,file,md=3,ne=1000,g=5,lr=.1):\n",
    "    try:\n",
    "        predict2=predict\n",
    "        predict=data[predict]\n",
    "        data=data.drop(columns=predict2)\n",
    "    except:\n",
    "        True \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, predict, test_size=0.33, random_state=42)\n",
    "    clf=trainxgboost(X_train,cols,y_train)\n",
    "    \n",
    "    predict=clf.predict_proba(X_test[clf.get_booster().feature_names])\n",
    "    X_test['Actual']=y_test\n",
    "    writeperformance(X_test,predict[:,1],'Actual',file)\n",
    "\n",
    "    \n",
    "    \n",
    "#rturn plot of feature importance\n",
    "def Graphfeatures(clf,x):\n",
    "    import numpy.core.multiarray\n",
    "    import numpy\n",
    "    temp=clf.booster().get_score(importance_type='gain')\n",
    "    explainer=shap.TreeExplainer(clf)\n",
    "    shap_values = explainer.shap_values(x)\n",
    "    pic=shap.summary_plot(shap_values, x,feature_names=xvec.get_feature_names(),max_display=200)\n",
    "    print(pic)\n",
    "\n",
    "\n",
    "    \n",
    "# predict binary classification after cleaning data\n",
    "def completebasiccyclebinary(data,predict,targetlist,top,mincount,file,weightfile,md=3,ne=1000,g=5,lr=.1):\n",
    "\n",
    "    testing= (getfeaturesback(data,predict,targetlist,weightfile,top))\n",
    "    testing=commonfeatures(testing)\n",
    "    cols=minocc(testing,mincount)\n",
    "    splittest(data,cols,predict,file,md,ne,g,lr)\n",
    "    \n",
    "    return (True)\n",
    "\n",
    "\n",
    "#find groupings of values making buckets between min and max amounts of buckets\n",
    "def groupingsten(data,col,maxbucket,minbucket):\n",
    "    testnum=data[col].quantile(.9)-data[col].quantile(.1)\n",
    "    l=len(str(testnum))\n",
    "    divide=10/(pow(10,l))\n",
    "    while (testnum/divide>=maxbucket or testnum/divide<=minbucket):\n",
    "        divide=divide*10\n",
    "    return divide\n",
    "\n",
    "\n",
    "\n",
    "#divides into groups of of second nearest 10 so if 200 then 10-20,20-30,120-130, if 10 then 1 2 3 4 5, if 8.0 then .1,.2,.3,.4,7.7-7.8\n",
    "def standarddivide(data,col,target):\n",
    "    data=data[[col,target]]\n",
    "    data[col]=data[col].astype(float)\n",
    "    data[target]=data[target].astype(float)\n",
    "    # gets groupings between number of buckets\n",
    "    divide=groupingsten(data,col,100,10)\n",
    "    mini=data[col].min()\n",
    "    maxi=data[col].max()\n",
    "    while mini<maxi+divide:\n",
    "        data[str(mini)+\"-\"+str(mini+divide)]=0\n",
    "        mask=((data[col]>=mini) & (data[col]<=mini+divide))\n",
    "        data.loc[mask,str(mini)+\"-\"+str(mini+divide)]=1\n",
    "        mini=mini+divide\n",
    "    data=data.drop(columns=col)\n",
    "    score=corrolationtopaverage(data,target,.3)\n",
    "    return data,score\n",
    "\n",
    " # tries all amount of bins using pandas qcut from minbucket to maxbucket amount and scores the correlation of the top percent\n",
    "# finds the best amount of buckets for that continuous column    \n",
    "def bestqcut(data,col,target,minbucket,maxbucket,percent):\n",
    "    #print (data)\n",
    "    data=data[[col,target]]\n",
    "    bestbuckets=0\n",
    "    bestscore=0\n",
    "    while minbucket<maxbucket:\n",
    "        temptest=list(set(list(pd.qcut(data[col],q=minbucket))))\n",
    "        l=len(temptest)\n",
    "        i=0\n",
    "        while i<l:\n",
    "            loc=str(temptest[i]).find(',')\n",
    "        \n",
    "            minc= float((str(temptest[i])[1:loc]))\n",
    "            maxc= float((str(temptest[i])[loc+2:-1]))\n",
    "            mask=((data[col]>=minc) & (data[col]<=maxc))\n",
    "            data[str(minc)+'_'+str(maxc)]=0\n",
    "            data.loc[mask,str(minc)+'_'+str(maxc)]=1\n",
    "            \n",
    "            i=i+1\n",
    "        data2=data.drop(columns=col)\n",
    "        score=corrolationtopaverage(data2,target,percent)\n",
    "        if score>bestscore:\n",
    "            bestscore=score\n",
    "            bestbuckets=minbucket\n",
    "        minbucket=minbucket+1\n",
    "    return bestscore,bestbuckets\n",
    "        \n",
    "\n",
    " # tries all amount of bins using pandas quantile from minbucket to maxbucket amount and scores the correlation of the top percent\n",
    "# finds the best amount of buckets for that continuous column        \n",
    "def bestquant(data,col,target,minbucket,maxbucket,percent):\n",
    "    data=data[[col,target]]\n",
    "    bestbucket=0\n",
    "    bestscore=0\n",
    "    while minbucket<maxbucket:\n",
    "        moving=1/minbucket\n",
    "        minc=0\n",
    "        while minc<1:\n",
    "            if minc+moving>=100:\n",
    "                data[str(minc)+'_1']=0\n",
    "                mask=data[col]>data[col].quantile(minc)\n",
    "                data.loc[mask,str(minc)+'_1']=1\n",
    "            \n",
    "            data[str(minc)+'_'+str(minc+moving)]=0\n",
    "            mask=((data[col]>=data[col].quantile(minc)) & (data[col]<=data[col].quantile(minc+moving)))\n",
    "            data.loc[str(minc)+'_'+str(minc+moving)]=1\n",
    "            minc=minc+1\n",
    "        data2=data.drop(columns=col)\n",
    "        score=corrolationtopaverage(data2,target,percent)\n",
    "        if score>bestscore:\n",
    "            bestscore=score\n",
    "            bestbuckets=minbucket\n",
    "        minbucket=minbucket+1\n",
    "    return bestscore,bestbuckets\n",
    "                \n",
    "            \n",
    "            \n",
    "    \n",
    " # tries all amount of bins using pandas.cut from minbucket to maxbucket amount and scores the correlation of the top percent\n",
    "# finds the best amount of cuts for that continuous column\n",
    "def bestcut(data,col,target,minbucket,maxbucket,percent):\n",
    "    data=data[[col,target]]\n",
    "    bestbuckets=0\n",
    "    bestscore=0\n",
    "    while minbucket<maxbucket:\n",
    "        temptest=list(set(list(pd.cut(data[col],bins=minbucket))))\n",
    "        l=len(temptest)\n",
    "        i=0\n",
    "        while i<l:\n",
    "            loc=str(temptest[i]).find(',')\n",
    "        \n",
    "            minc= float((str(temptest[i])[1:loc]))\n",
    "            maxc= float((str(temptest[i])[loc+2:-1]))\n",
    "            mask=((data[col]>=minc) & (data[col]<=maxc))\n",
    "            data[str(minc)+'_'+str(maxc)]=0\n",
    "            data.loc[mask,str(minc)+'_'+str(maxc)]=1\n",
    "            \n",
    "            i=i+1\n",
    "        data2=data.drop(columns=col)\n",
    "        score=corrolationtopaverage(data2,target,percent)\n",
    "        if score>bestscore:\n",
    "            bestscore=score\n",
    "            bestbuckets=minbucket\n",
    "        minbucket=minbucket+1\n",
    "    return bestscore,bestbuckets\n",
    "\n",
    "\n",
    "#return the average correlation for the top percentage of features\n",
    "def corrolationtopaverage(data,target,percent):\n",
    "    data=data.corr()\n",
    "    cor=list(abs(data[target][:].values)).sort(reverse=False).pop().sort(reverse=True)\n",
    "    if int(len(cor)*percent) <1:\n",
    "        amount=1\n",
    "    else:\n",
    "        amount=int(len(cor)*percent)\n",
    "    return ( statistics.mean(cor[:amount]))\n",
    "\n",
    "\n",
    "# after finding the best cut the column into those categories\n",
    "def cutting(data,col,cuttype,cutbucket):\n",
    "\n",
    "    if cuttype=='QCUT' or cuttype=='CUT':\n",
    "        if cuttype=='QCUT':\n",
    "            temptest=list(set(list(pd.qcut(data[col],q=cutbucket))))\n",
    "        else:\n",
    "            temptest=list(set(list(pd.cut(data[col],bins=cutbucket))))\n",
    "        while cols in temptest:\n",
    "            loc=str(cols).find(',')\n",
    "            minc= float((str(cols)[1:loc]))\n",
    "            maxc= float((str(cols)[loc+2:-1]))\n",
    "            mask=((data[col]>=minc) & (data[col]<=maxc))\n",
    "            data[str(col)+'_'+str(minc)+'_'+str(maxc)]=0\n",
    "            data.loc[mask,str(col)+'_'+str(minc)+'_'+str(maxc)]=1\n",
    "            i=i+1\n",
    "        data=data.drop(columns=col)\n",
    "        return data\n",
    "    if cuttype=='QUANT':\n",
    "        \n",
    "        moving=1/cutbucket\n",
    "        minc=0\n",
    "        while minc<1:\n",
    "            if minc+moving>=100:\n",
    "                data[str(col)+'_'+str(minc)+'_1']=0\n",
    "                mask=data[col]>data[col].quantile(minc)\n",
    "                data.loc[mask,str(col)+'_'+str(minc)+'_1']=1\n",
    "            data[str(col)+'_'+str(minc)+'_'+str(minc+moving)]=0\n",
    "            mask=((data[col]>=data[col].quantile(minc)) & (data[col]<=data[col].quintile(minc+moving)))\n",
    "            data.loc[mask,str(col)+'_'+str(minc)+'_'+str(minc+moving)]=1\n",
    "            minc=minc+1\n",
    "        data=data.drop(columns=col)\n",
    "        return data\n",
    "\n",
    "    \n",
    "#testing the best way to cut the column, and then decides how many categories to cut it into\n",
    "# if category wont be better for continuous  then keeps it as is\n",
    "def bestcutselect(data,col,target,minbucket,maxbucket,percent):\n",
    "    #finds best  amount based on correlation with target\n",
    "    qcutscore,qcutbucket=bestqcut(data,col,target,minbucket,maxbucket,percent)\n",
    "    quantscore,quantbucket=bestquant(data,col,target,minbucket,maxbucket,percent)\n",
    "    cutscore,cutbucket=bestcut(data,col,target,minbucket,maxbucket,percent)\n",
    "    bestscore=corrolationtopaverage(data[[col,target]],target,percent)\n",
    "    temp,standardscore=standarddivide(data[[col,target]],col,target)\n",
    "    # which best cut amount worked best\n",
    "    if ((qcutscore>=quantscore) and (qcutscore>=cutscore) and (qcutscore>bestscore)and (qcutscore>=standardscore)):\n",
    "        return cutting(data,col,'QCUT',qcutbucket)\n",
    "    if ((quantscore>=qcutscore) and (quantscore>=cutscore)and (quantscore>bestscore)and (quantscore>=standardscore)):\n",
    "        return cutting(data,col,'QUANT',qcutbucket)\n",
    "    if ((cutscore>=qcutscore) and (cutscore>=quantscore)and (cutscore>bestscore) and (cutscore>=standardscore)):\n",
    "        return cutting(data,col,'CUT',qcutbucket)\n",
    "    if ((standardscore>=qcutscore)and (standardscore>=cutscore) and (standardscore>=quantscore)and (standardscore>bestscore)):\n",
    "        temp,standscore=standarddivide(data[[col,target]],col,target)\n",
    "        return temp\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "#Calculate average length of text\n",
    "def averageLen(lst):\n",
    "    lengths = [len(str(i)) for i in lst]\n",
    "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths))\n",
    "\n",
    "\n",
    "#figures out if column is a category or string column\n",
    "def catorstring(data,catlist,minlength=30):\n",
    "    strlist=[]\n",
    "    for cat in catlist:\n",
    "        # this is used based on average length of text to tell if string or category\n",
    "        #if averageLen(list(data[cat].value_counts().index))>minlength:\n",
    "        #this is ued based on amount of unique values in the column to tell if free text or category\n",
    "        if len(data[cat].value_counts().index)>minlength:\n",
    "            strlist.append(cat)\n",
    "            catlist.remove(cat)\n",
    "    return catlist,strlist\n",
    "\n",
    "\n",
    "#figures out if its really a number or text/category column\n",
    "def intorcatclean(data,cols,intlist=[],catlist=[]):\n",
    "    for col in cols:\n",
    "        # if after after keeping numbers only there are more than 3 unique values then actually number\n",
    "        if len(data[col].astype(str).str.replace(\"\\d+.\", \"\").str.replace(\".\",'').value_counts().index) > 3:\n",
    "            catlist.append(col)\n",
    "        else :\n",
    "            intlist.append(col)\n",
    "    return intlist,catlist\n",
    "\n",
    "\n",
    "\n",
    "# add columns to data based on text vectorizer sent, TFIDF and Count work\n",
    "def vectorizertopandas(data,cols,vectorizer,addon=''):\n",
    "    xvec=vectorizer.fit(data[cols])\n",
    "    xll=xvec.transform(data[cols])\n",
    "    #vectorizer and turn vector data into dense dataframe\n",
    "    xll2=pd.DataFrame(xll.todense(),columns=xvec.get_feature_names())\n",
    "    # add columns based on clean words/text\n",
    "    xll2.columns = [(col).encode('utf-8').strip() + (addon).encode('utf-8').strip() for col in xll2.columns]\n",
    "    #add columns to original data\n",
    "    data=pd.concat([data,xll2],axis=1)\n",
    "    return(data.drop(columns=cols))\n",
    "\n",
    "\n",
    "#transform columns based on what is found\n",
    "def fixdatatypes(data,catcols,intcols,strcols,vectorizer):\n",
    "    #create dummies based off of category columns\n",
    "    data=pd.get_dummies(data, prefix=catcols, columns=catcols)\n",
    "    #transforms int columns into only numbers, cleans up dirty data\n",
    "    for col in intcols:\n",
    "        data[col]=data[col].astype(str)\n",
    "        data[col]=data[col].str.replace(\"[^\\d\\.]\", \"\")\n",
    "        data[col]=data[col].str.strip()\n",
    "        mask=data[col]==''\n",
    "        data.loc[mask,col]=-1\n",
    "        data[col]=data[col].str.strip()\n",
    "        data[col]=data[col].astype(float,errors='ignore')\n",
    "    # creates word columns from text\n",
    "    for col in strcols:     \n",
    "        data=vectorizertopandas(data,col,vectorizer,str(col+'_'))\n",
    "    return data\n",
    "\n",
    "\n",
    "def simplecompleterun(data,target,outputfile,weightfile,targetlist=['Positive'],top=20,mincount=1,minbucket=3,maxbucket=10,percent=.30,vectorizer=CountVectorizer( ngram_range=(1,4), max_df=.9999,min_df=.001)):\n",
    "    \n",
    "    #list of number columns\n",
    "    intlist=[]\n",
    "    #list of text columns\n",
    "    catlist=[]\n",
    "    #make dataframe without the target column\n",
    "    data2=data.drop(columns=target)\n",
    "    #text or number\n",
    "    intlist,catlist=intorcatclean(data,list(data2.columns.values))\n",
    "    #finds out if text columns is category or string category\n",
    "    catlist,strlist=catorstring(data,catlist)\n",
    "    \n",
    "    #clean up number columns and create columns based off free text\n",
    "    data=fixdatatypes(data,catlist,intlist,strlist,vectorizer)\n",
    "    l=len(intlist)\n",
    "    i=0\n",
    "    while i<l:\n",
    "        \n",
    "        data=bestcutselect(data,intlist[i],target,minbucket,maxbucket,percent)\n",
    "        i=i+1\n",
    "    completebasiccyclebinary(data,target,targetlist,top,mincount,outputfile,weightfile)\n",
    "    print ('Finished')\n",
    "    #print (data.head())\n",
    "    #print (data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    2.0\n",
       "2    3.3\n",
       "3      4\n",
       "4      5\n",
       "Name: Mix, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
